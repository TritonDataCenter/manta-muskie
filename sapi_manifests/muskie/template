{
  "clearProxyPort": 81,
  "bunyan": {
    "level": "info",
    "syslog": {
      "facility": "local0",
      "type": "udp"
    }
  },
  "maxObjectCopies": 6,
  "maxRequestAge": 600,
  "numWorkers": 4,
  "port": 80,
  "auth": {
    "url": "http://{{AUTH_SERVICE}}",
    "maxAuthCacheSize": 1000,
    "maxAuthCacheAgeMs": 300000,
    "maxTranslationCacheSize": 1000,
    "maxTranslationCacheAgeMs": 300000
  },
  "moray": {
    "morayOptions": {
        "srvDomain": "{{ELECTRIC_MORAY}}",
        "cueballOptions": {
            "resolvers": ["nameservice.{{DOMAIN_NAME}}"]
        }
    }
  },
  "marlin": {
    "moray": {
        "srvDomain": "{{MARLIN_MORAY_SHARD}}",
        "cueballOptions": {
            "resolvers": ["nameservice.{{DOMAIN_NAME}}"]
        }
    },
    "jobCache": {
      "size": 500,
      "expiry": 30
    }
  },

  {{!
    These settings are used for the generic cueball HttpAgent. This is used
    for talking to all HTTP APIs (in muskie's case, this means mahi)
  }}
  "cueballHttpAgent": {

    {{! Use "bootstrap" mode to find binder, then use it for lookups. }}
    "resolvers": ["nameservice.{{DOMAIN_NAME}}"],

    {{! Pre-populate pools to make startup faster. }}
    "initialDomains": [
      "{{AUTH_SERVICE}}"
    ],

    {{!
      Make sure that if a socket is left idle for >60sec, a dummy "ping"
      request is made on it. This stops the node.js 2-minute server socket
      timeout on the authcache side from closing our sockets (and potentially
      racing with our use of that socket).
    }}
    "ping": "/ping",
    "pingInterval": 60000,

    {{!
      Separately, we want to enable TCP-level keep-alives on the sockets, to
      ensure that we notice quickly if an entire CN running an authcache panics
      or is netsplit from us, to avoid handing it requests that will fail.
    }}
    "tcpKeepAliveInitialDelay": 10000,

    {{!
      The spares value here should be larger than sharkConfig.spares, by
      a factor probably >2, <10. Mahi connections are shared amongst all reqs
      unlike shark connections, so we need more of them.
    }}
    "spares": 8,
    {{!
      We never want to hit this cap, so make it plenty big. This value (200) is
      an order of magnitude above the max socket usage seen in lab tests.
    }}
    "maximum": 200,

    "recovery": {
      "default": {
        {{!
          Values less than 2s seem to yield lots of false failures. Cueball
          will double the timeout until it hits maxTimeout.
        }}
        "timeout": 2000,
        "maxTimeout": 10000,
        {{!
          Number of retries until the backend is declared "dead". 3-5 seems to
          work well in SDC.
        }}
        "retries": 5,
        {{! Delay between retries, to space them out. }}
        "delay": 250,
        "maxDelay": 2000
      },

      {{!
        No retries on DNS SRV lookups, because mahi currently registers itself
        in DNS as a redis service, not HTTP. See MANTA-3017 and related bugs.
      }}
      "dns_srv": {
        "timeout": 2000,
        "maxTimeout": 10000,
        "retries": 1,
        "delay": 0,
        "maxDelay": 0
      }
    }
  },

  {{!
    These settings are used to set up the per-shark cueball agent. This
    manages connections from the muskie out to makos for storing/retrieving
    actual object data.
  }}
  "sharkConfig": {
    {{! These are translated into cueball recovery parameters. }}
    "connectTimeout": 2000,
    "maxTimeout": 30000,
    "delay": 500,
    "retry": {
      "retries": 5
    },

    {{! Use bootstrap mode to find binder, same as for cueballHttpAgent. }}
    "resolvers": ["nameservice.{{DOMAIN_NAME}}"],

    {{! TCP keepalive initial timeout. }}
    "maxIdleTime": 10000,
    "maxClients": 50,

    {{! nginx has much longer max idle times than node.js }}
    "ping": "/ping",
    "pingInterval": 14400000,

    {{!
      We want spares to be small: for the sake of prudence we don't want lots
      of idle sockets sitting around for no reason. We also want it to be large:
      we want to be able to have a margin to soak up load transients. It was
      originally proposed that this be set to 1, but this causes suffering under
      transient load. The value 2 seems to work ok in the lab.
    }}
    "spares": 2,
    {{!
      This needs to be very high, as some requests to sharks may take a very
      long time to complete (and this is normal). We need plenty of headroom
      to make sure we don't queue requests unnecessarily.
    }}
    "maximum": 2000
  },

  {{#MPU_ENABLE}}
  "enableMPU": true,
  {{/MPU_ENABLE}}

  "medusa": {
    "moray": {
      "srvDomain": "{{ELECTRIC_MORAY}}",
      "cueballOptions": {
          "resolvers": ["nameservice.{{DOMAIN_NAME}}"]
      }
    },
    "reflector": {
      "host": "{{MEDUSA_REFLECTOR}}",
      "port": 8381
    }
  },
  "storage": {
    "lag": 60000,
    "multiDC": {{MUSKIE_MULTI_DC}}{{#MUSKIE_DEFAULT_MAX_STREAMING_SIZE_MB}},
    "defaultMaxStreamingSizeMB": {{MUSKIE_DEFAULT_MAX_STREAMING_SIZE_MB}}{{/MUSKIE_DEFAULT_MAX_STREAMING_SIZE_MB}},
    "moray": {
        "srvDomain": "{{STORAGE_MORAY_SHARD}}",
        "cueballOptions": {
            "resolvers": ["nameservice.{{DOMAIN_NAME}}"]
        }
    }
  },
  "authToken": {
    "salt": "{{MUSKIE_JOB_TOKEN_AES_SALT}}",
    "key": "{{MUSKIE_JOB_TOKEN_AES_KEY}}",
    "iv": "{{MUSKIE_JOB_TOKEN_AES_IV}}",
    "maxAge": 604800000
  },
  "ufds": {
      "url": "{{{UFDS_URL}}}",
      "bindDN": "{{{UFDS_ROOT_DN}}}",
      "bindPassword": "{{{UFDS_ROOT_PW}}}",
      "cache": {
          "size": 5000,
          "expiry": 60
      },
      "maxConnections": 1,
      "retry": {
          "initialDelay": 1000
      },
      "clientTimeout": 120000
  }

}
